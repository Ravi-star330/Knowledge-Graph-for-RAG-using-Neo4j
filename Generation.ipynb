{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "import textwrap\n",
    "\n",
    "# Langchain\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from environment\n",
    "load_dotenv('.env', override=True)\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "NEO4J_DATABASE = os.getenv('NEO4J_DATABASE') or 'neo4j'\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "OPENAI_ENDPOINT = os.getenv('OPENAI_BASE_URL') + '/embeddings'\n",
    "\n",
    "\n",
    "\n",
    "# Global constants\n",
    "VECTOR_INDEX_NAME = 'NapoleonOpenAI'\n",
    "VECTOR_NODE_LABEL = 'Napoleon_Chunk'\n",
    "VECTOR_SOURCE_PROPERTY = 'text'\n",
    "VECTOR_EMBEDDING_PROPERTY = 'textEmbeddingOpenAI'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = Neo4jGraph(\n",
    "    url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg.query(\"\"\"\n",
    "         CREATE VECTOR INDEX `NapoleonOpenAI` IF NOT EXISTS\n",
    "          FOR (nc:Napoleon_Chunk) ON (nc.textEmbeddingOpenAI) \n",
    "          OPTIONS { indexConfig: {\n",
    "            `vector.dimensions`: 1536,\n",
    "            `vector.similarity_function`: 'cosine'    \n",
    "         }}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "# kg.query(\"\"\"DROP INDEX `Napoleon`\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 4,\n",
       "  'name': 'Napoleon',\n",
       "  'state': 'ONLINE',\n",
       "  'populationPercent': 100.0,\n",
       "  'type': 'VECTOR',\n",
       "  'entityType': 'NODE',\n",
       "  'labelsOrTypes': ['Napoleon_Chunk'],\n",
       "  'properties': ['textEmbedding'],\n",
       "  'indexProvider': 'vector-2.0',\n",
       "  'owningConstraint': None,\n",
       "  'lastRead': neo4j.time.DateTime(2024, 7, 25, 21, 15, 57, 658000000, tzinfo=<UTC>),\n",
       "  'readCount': 20},\n",
       " {'id': 5,\n",
       "  'name': 'NapoleonOpenAI',\n",
       "  'state': 'ONLINE',\n",
       "  'populationPercent': 100.0,\n",
       "  'type': 'VECTOR',\n",
       "  'entityType': 'NODE',\n",
       "  'labelsOrTypes': ['Napoleon_Chunk'],\n",
       "  'properties': ['textEmbeddingOpenAI'],\n",
       "  'indexProvider': 'vector-2.0',\n",
       "  'owningConstraint': None,\n",
       "  'lastRead': neo4j.time.DateTime(2024, 7, 29, 18, 22, 39, 777000000, tzinfo=<UTC>),\n",
       "  'readCount': 70}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg.query(\"\"\"\n",
    "  SHOW VECTOR INDEXES\n",
    "  \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG with Relationship Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_qa_chat_prompt = \"\"\"\n",
    "Task:Generate Cypher statement to \n",
    "query a graph database.\n",
    "Instructions:\n",
    "Use only the provided relationship types and properties in the \n",
    "schema. Do not use any other relationship types or properties that\n",
    "are not provided.\n",
    "Remember the relationships are like Schema:\n",
    "{schema}\n",
    "if question say Talleyrand it menas Charles-Maurice de Talleyrand \n",
    "and if say Napoleon means Napoleon Bonaparte and if say waterloo is Battle of Waterloo.\n",
    "\n",
    "Note: Do not include any explanations or apologies in your responses.\n",
    "Do not include any text except the generated Cypher statement. Remember to correct the typo in names\n",
    "\n",
    "Example 1: What was the story of napoleon in the battle of waterloo?\n",
    "MATCH (Napoleon:Person)-[:RELATED_TO]->(waterloo:Event)-[:HAS_General_INFO]->(info:General_info)-[:HAS_Chunk_INFO]->(ChunkInfo:Waterloo_Chunk)\n",
    "RETURN p, e, info, ChunkInfo.text\n",
    "\n",
    "Example 2: What was the story of the battle of waterloo?\n",
    "MATCH (waterloo:Event)-[:HAS_General_INFO]->(info:General_info)-[:HAS_Chunk_INFO]->(ChunkInfo:Waterloo_Chunk)\n",
    "RETURN p, e, info, ChunkInfo.text\n",
    "\n",
    "Example 3: tell me about Talleyrand and napoleon in 5 lines\n",
    "MATCH (Talleyrand:Person)-[:RELATED_TO]->(Napoleon:Person)-[:HAS_Career_INFO]->(info:Career_info)-[:HAS_Chunk_INFO]->(ChunkInfo:Napoleon_Chunk)\n",
    "RETURN Talleyrand, Napoleon\n",
    "\n",
    "The question is:\n",
    "{question}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"], \n",
    "    template=retrieval_qa_chat_prompt\n",
    ")\n",
    "\n",
    "cypherChain = GraphCypherQAChain.from_llm(\n",
    "    ChatOpenAI(temperature=0),\n",
    "    graph=kg,\n",
    "    verbose=True,\n",
    "    cypher_prompt=CYPHER_GENERATION_PROMPT,\n",
    ")\n",
    "\n",
    "def prettyCypherChain(question: str) -> str:\n",
    "    response = cypherChain.run(question)\n",
    "    print(textwrap.fill(response, 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectorRAG without Relationship Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated procedure. ('db.create.setVectorProperty' has been replaced by 'db.create.setNodeVectorProperty')} {position: line: 1, column: 76, offset: 75} for query: \"UNWIND $data AS row MATCH (n:`Napoleon_Chunk`) WHERE elementId(n) = row.id CALL db.create.setVectorProperty(n, 'textEmbeddingOpenAI', row.embedding) YIELD node RETURN count(*)\"\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "\n",
    "vector_store = Neo4jVector.from_existing_graph(\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    index_name=VECTOR_INDEX_NAME,\n",
    "    node_label=VECTOR_NODE_LABEL,\n",
    "    text_node_properties=[VECTOR_SOURCE_PROPERTY],\n",
    "    embedding_node_property=VECTOR_EMBEDDING_PROPERTY,\n",
    ")\n",
    "\n",
    "\n",
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "combine_docs_chain = create_stuff_documents_chain(ChatOpenAI(temperature=0), retrieval_qa_chat_prompt)\n",
    "retrival_chain = create_retrieval_chain(\n",
    "    retriever=vector_store.as_retriever(), combine_docs_chain=combine_docs_chain\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (waterloo:Event)-[:HAS_Combatant_INFO]->(combatant:Combatant)\n",
      "RETURN combatant.frenchCommander\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'combatant.frenchCommander': 'Napoleon Bonaparte'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Napoleon Bonaparte was leading the Battle of Waterloo.\n"
     ]
    }
   ],
   "source": [
    "# Vector similarity search With Relationship \n",
    "prettyCypherChain(\"who was leading battle of waterloo?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wellington was leading the Battle of Waterloo.\n"
     ]
    }
   ],
   "source": [
    "# Simple Vector similarity search without considering Relationship between nodes\n",
    "query = {\"input\": \"who was leading battle of waterloo?\"}\n",
    "\n",
    "result = retrival_chain.invoke(input=query)\n",
    "print(textwrap.fill(result['answer'], 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
